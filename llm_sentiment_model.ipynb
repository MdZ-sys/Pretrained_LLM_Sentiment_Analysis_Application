{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0868bef-da03-4378-897c-52fb9ea62336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting transformers\n  Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\nCollecting sentencepiece\n  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nRequirement already satisfied: torch in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages (1.8.1)\nCollecting torch\n  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\nCollecting langchain\n  Downloading langchain-0.0.281-py3-none-any.whl (1.6 MB)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from transformers) (21.3)\nCollecting pyyaml>=5.1\n  Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\nCollecting safetensors>=0.3.1\n  Using cached safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\nRequirement already satisfied: numpy>=1.17 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages (from transformers) (1.24.3)\nCollecting regex!=2019.12.17\n  Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\nCollecting huggingface-hub<1.0,>=0.15.1\n  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\nCollecting tqdm>=4.27\n  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from transformers) (2.27.1)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from torch) (2.11.3)\nCollecting nvidia-curand-cu11==10.2.10.91\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\nCollecting nvidia-cusparse-cu11==11.7.4.91\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\nCollecting nvidia-cublas-cu11==11.10.3.66\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\nCollecting nvidia-cusolver-cu11==11.4.0.1\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\nCollecting nvidia-nccl-cu11==2.14.3\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\nCollecting triton==2.0.0\n  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.9/site-packages (from torch) (4.1.1)\nCollecting sympy\n  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\nCollecting nvidia-nvtx-cu11==11.7.91\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\nCollecting networkx\n  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\nCollecting nvidia-cudnn-cu11==8.5.0.96\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\nRequirement already satisfied: wheel in /databricks/python3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (61.2.0)\nCollecting lit\n  Downloading lit-16.0.6.tar.gz (153 kB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Installing backend dependencies: started\n  Installing backend dependencies: finished with status 'done'\n    Preparing wheel metadata: started\n    Preparing wheel metadata: finished with status 'done'\nCollecting cmake\n  Downloading cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\nCollecting numexpr<3.0.0,>=2.8.4\n  Downloading numexpr-2.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\nCollecting SQLAlchemy<3,>=1.4\n  Using cached SQLAlchemy-2.0.20-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\nCollecting tenacity<9.0.0,>=8.1.0\n  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\nCollecting langsmith<0.1.0,>=0.0.21\n  Downloading langsmith-0.0.33-py3-none-any.whl (36 kB)\nCollecting pydantic<3,>=1\n  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\nCollecting dataclasses-json<0.6.0,>=0.5.7\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nCollecting aiohttp<4.0.0,>=3.8.3\n  Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\nCollecting async-timeout<5.0.0,>=4.0.0\n  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting frozenlist>=1.1.1\n  Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\nCollecting yarl<2.0,>=1.0\n  Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\nCollecting aiosignal>=1.1.2\n  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting multidict<7.0,>=4.5\n  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\nCollecting typing-inspect<1,>=0.4.0\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nCollecting marshmallow<4.0.0,>=3.18.0\n  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\nCollecting fsspec\n  Using cached fsspec-2023.9.0-py3-none-any.whl (173 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\nCollecting annotated-types>=0.4.0\n  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\nCollecting typing-extensions\n  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nCollecting pydantic-core==2.6.3\n  Downloading pydantic_core-2.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\nCollecting greenlet!=0.4.17\n  Using cached greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\nRequirement already satisfied: MarkupSafe>=0.23 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\nCollecting mpmath>=0.19\n  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (PEP 517): started\n  Building wheel for lit (PEP 517): finished with status 'done'\n  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93603 sha256=58a69e84bcbf0d8ce71632ad42aa155c28c4909f1f651de91d30f4effa4361ae\n  Stored in directory: /root/.cache/pip/wheels/a5/36/d6/cac2e6fb891889b33a548f2fddb8b4b7726399aaa2ed32b188\nSuccessfully built lit\nInstalling collected packages: typing-extensions, pydantic-core, multidict, frozenlist, annotated-types, yarl, typing-inspect, tqdm, pyyaml, pydantic, nvidia-cublas-cu11, mpmath, marshmallow, lit, greenlet, fsspec, cmake, async-timeout, aiosignal, triton, tokenizers, tenacity, sympy, SQLAlchemy, safetensors, regex, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-cusolver-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, numexpr, networkx, langsmith, huggingface-hub, dataclasses-json, aiohttp, transformers, torch, sentencepiece, langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4101a58a-00cc-4444-ba06-d08e941ca95c\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.0.1\n    Not uninstalling tenacity at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4101a58a-00cc-4444-ba06-d08e941ca95c\n    Can't uninstall 'tenacity'. No files were found to uninstall.\n  Attempting uninstall: torch\n    Found existing installation: torch 1.8.1\n    Not uninstalling torch at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-4101a58a-00cc-4444-ba06-d08e941ca95c\n    Can't uninstall 'torch'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\nSuccessfully installed SQLAlchemy-2.0.20 aiohttp-3.8.5 aiosignal-1.3.1 annotated-types-0.5.0 async-timeout-4.0.3 cmake-3.27.4.1 dataclasses-json-0.5.14 frozenlist-1.4.0 fsspec-2023.9.0 greenlet-2.0.2 huggingface-hub-0.16.4 langchain-0.0.281 langsmith-0.0.33 lit-16.0.6 marshmallow-3.20.1 mpmath-1.3.0 multidict-6.0.4 networkx-3.1 numexpr-2.8.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pydantic-2.3.0 pydantic-core-2.6.3 pyyaml-6.0.1 regex-2023.8.8 safetensors-0.3.3 sentencepiece-0.1.99 sympy-1.12 tenacity-8.2.3 tokenizers-0.13.3 torch-2.0.1 tqdm-4.66.1 transformers-4.33.0 triton-2.0.0 typing-extensions-4.7.1 typing-inspect-0.9.0 yarl-1.9.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentencepiece torch --upgrade langchain pydantic==1.8 mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499a7eaf-7fe2-407d-a5ce-527e41733827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.pytorch\n",
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "929923d9-98ac-402f-b0e2-8362c4ce96a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14659d34bea49779ef2c2a9620707fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde14d9ad9664d288e233733326762f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2242cc4eb5f64c2cbde56d21b6254d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3947e2d9de04faab2d62c15b9f34ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb356db7b894988b3b85281585828f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9b22840aa74771946a1ce643ec8a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d37e0768fc47699d7fa41733d91fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a855aa89b10643fabe8b1ec299c6f610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187938b43c64622bae5cb485dae8bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the sentiment analysis models\n",
    "model_1 = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "model_2 = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)\n",
    "test_data = pd.read_csv('/dbfs/FileStore/tables/test_df.csv')\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df = pd.get_dummies(test_data, columns=['sentiment'], prefix='', prefix_sep='')\n",
    "\n",
    "# Define a function to evaluate a model on the dataset\n",
    "def evaluate_model(texts, model):\n",
    "    sentiment_labels = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Ensure that 'text' is a string, or convert it to a string if it's not\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        # Get the model's predictions\n",
    "        prediction = model(text)\n",
    "        sentiment_label = prediction[0]['label']\n",
    "\n",
    "        sentiment_labels.append(sentiment_label)\n",
    "\n",
    "    return sentiment_labels\n",
    "\n",
    "# Evaluate \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" alone\n",
    "sentiment_labels_1 = evaluate_model(df['text'], model_1)\n",
    "\n",
    "# Evaluate \"cardiffnlp/twitter-roberta-base-sentiment-latest\" alone\n",
    "sentiment_labels_2 = evaluate_model(df['text'], model_2)\n",
    "\n",
    "# Define a function to perform ensemble evaluation\n",
    "def ensemble_evaluate(texts, model_1, model_2):\n",
    "    sentiment_labels_ensemble = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Ensure that 'text' is a string, or convert it to a string if it's not\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        # Get predictions from both models\n",
    "        prediction_1 = model_1(text)\n",
    "        prediction_2 = model_2(text)\n",
    "\n",
    "        # Combine the predictions using a weighted average or majority vote\n",
    "        sentiment_score_1 = prediction_1[0]['score']\n",
    "        sentiment_score_2 = prediction_2[0]['score']\n",
    "        combined_sentiment_score = (sentiment_score_1 + sentiment_score_2) / 2\n",
    "\n",
    "        # Choose the label based on the combined score\n",
    "        combined_sentiment_label = prediction_1[0]['label'] if combined_sentiment_score >= 0.5 else prediction_2[0]['label']\n",
    "\n",
    "        sentiment_labels_ensemble.append(combined_sentiment_label)\n",
    "\n",
    "    return sentiment_labels_ensemble\n",
    "\n",
    "# Evaluate both models in an ensemble fashion\n",
    "sentiment_labels_ensemble = ensemble_evaluate(df['text'], model_1, model_2)\n",
    "\n",
    "# Add the results as new columns in your DataFrame\n",
    "df['sentiment_label_1'] = sentiment_labels_1\n",
    "df['sentiment_label_2'] = sentiment_labels_2\n",
    "df['sentiment_label_ensemble'] = sentiment_labels_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d1b320-1991-41d6-a69f-1af086ec740e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Model 1:\n              precision    recall  f1-score   support\n\n    negative       0.63      0.88      0.73      1546\n     neutral       0.76      0.42      0.54      1930\n    positive       0.73      0.83      0.78      1730\n\n    accuracy                           0.69      5206\n   macro avg       0.70      0.71      0.68      5206\nweighted avg       0.71      0.69      0.68      5206\n\n\nClassification Report for Model 2:\n              precision    recall  f1-score   support\n\n    negative       0.69      0.82      0.75      1546\n     neutral       0.74      0.46      0.57      1930\n    positive       0.70      0.87      0.78      1730\n\n    accuracy                           0.70      5206\n   macro avg       0.71      0.72      0.70      5206\nweighted avg       0.71      0.70      0.69      5206\n\n\nClassification Report for Ensemble:\n              precision    recall  f1-score   support\n\n    negative       0.63      0.88      0.73      1546\n     neutral       0.75      0.42      0.54      1930\n    positive       0.73      0.83      0.78      1730\n\n    accuracy                           0.69      5206\n   macro avg       0.70      0.71      0.68      5206\nweighted avg       0.71      0.69      0.68      5206\n\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Calculate classification report for Model 1\n",
    "# report_model_1 = classification_report(test_data['sentiment'], df['sentiment_label_1'])\n",
    "\n",
    "# # Calculate classification report for Model 2\n",
    "# report_model_2 = classification_report(test_data['sentiment'], df['sentiment_label_2'])\n",
    "\n",
    "# # Calculate classification report for Ensemble\n",
    "# report_ensemble = classification_report(test_data['sentiment'], df['sentiment_label_ensemble'])\n",
    "\n",
    "# # Print the classification report for each model and ensemble\n",
    "# print(\"Classification Report for Model 1:\")\n",
    "# print(report_model_1)\n",
    "\n",
    "# print(\"\\nClassification Report for Model 2:\")\n",
    "# print(report_model_2)\n",
    "\n",
    "# print(\"\\nClassification Report for Ensemble:\")\n",
    "# print(report_ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4bce0b-0a2b-4a42-bb7f-b39cbe77e0cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Model 2\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Calculate classification report for Model 1\n",
    "# report_model_1 = classification_report(test_data['sentiment'], df['sentiment_label_1'], output_dict=True)\n",
    "\n",
    "# # Calculate classification report for Model 2\n",
    "# report_model_2 = classification_report(test_data['sentiment'], df['sentiment_label_2'], output_dict=True)\n",
    "\n",
    "# # Calculate classification report for Ensemble\n",
    "# report_ensemble = classification_report(test_data['sentiment'], df['sentiment_label_ensemble'], output_dict=True)\n",
    "\n",
    "# # Extract F1-scores for each class and each model\n",
    "# f1_scores_model_1 = report_model_1['weighted avg']['f1-score']\n",
    "# f1_scores_model_2 = report_model_2['weighted avg']['f1-score']\n",
    "# f1_scores_ensemble = report_ensemble['weighted avg']['f1-score']\n",
    "\n",
    "# # Compare F1-scores and select the best model\n",
    "# best_model = None\n",
    "\n",
    "# if f1_scores_model_1 > f1_scores_model_2 and f1_scores_model_1 > f1_scores_ensemble:\n",
    "#     best_model = \"Model 1\"\n",
    "# elif f1_scores_model_2 > f1_scores_ensemble:\n",
    "#     best_model = \"Model 2\"\n",
    "# else:\n",
    "#     best_model = \"Ensemble\"\n",
    "\n",
    "# print(\"Best Model:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d87fd5-d01e-484e-b29d-9a72b6870721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###################- Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f37176-d706-42c0-b158-2fba592dd526",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a715f869-a426-4afd-9caf-bdb812a8d755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.pytorch\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an MLflow experiment\n",
    "mlflow.set_experiment(\"/Users/mohamed.zahid@simpleenergy.in/sentiment_analysis\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        \"dataset_path\": \"/Users/mohamed.zahid@simpleenergy.in/sentiment_analysis\"\n",
    "    })\n",
    "\n",
    "    # Load the sentiment analysis model\n",
    "    model = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "\n",
    "    # Load your dataset\n",
    "    delta_df = spark.read.format(\"delta\").table(\"user_comments.comments\")\n",
    "    df = delta_df.toPandas()\n",
    "\n",
    "    # Define a function to analyze sentiment for a single text\n",
    "    def analyze_sentiment(text, model):\n",
    "        # Ensure that 'text' is a string\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # Get the sentiment prediction for the text\n",
    "        prediction = model(text)\n",
    "        \n",
    "        # Extract sentiment label and score\n",
    "        sentiment_label = prediction[0]['label']\n",
    "        sentiment_score = prediction[0]['score']\n",
    "        \n",
    "        return sentiment_label, sentiment_score\n",
    "\n",
    "    # Create empty lists to store sentiment labels and scores\n",
    "    sentiment_labels = []\n",
    "    sentiment_scores = []\n",
    "\n",
    "    # Apply the sentiment analysis function to each row of the DataFrame\n",
    "    for comment_text in df['comment_text']:\n",
    "        label, score = analyze_sentiment(comment_text, model)\n",
    "        sentiment_labels.append(label)\n",
    "        sentiment_scores.append(score)\n",
    "\n",
    "    # Add sentiment labels and scores as new columns to the DataFrame\n",
    "    df['sentiment_label'] = sentiment_labels\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "\n",
    "    # Log the DataFrame as an artifact\n",
    "    #mlflow.log_artifact(pd.DataFrame.to_csv(df), \"dbfs:/FileStore\")\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2319183-e51e-4055-b015-4b18d8d24e8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-13</td>\n",
       "      <td>why this looks like ather 450x</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.649711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-12</td>\n",
       "      <td>No competition with ola 😅</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.514803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>If everything goes well this will be a big hit...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.795603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>One of the oldest auto channel in YouTube</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.447022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>1and hlf yr bck I was thinking to tk franchise...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.397321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Nice🎉</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.644901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Cool</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.491287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>Hum first</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.476607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>1st..😂🎉</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.578617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>1st comment😂 1st view , 1st like 😂</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.613403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>comment_text</th>\n      <th>sentiment_label</th>\n      <th>sentiment_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-05-13</td>\n      <td>why this looks like ather 450x</td>\n      <td>negative</td>\n      <td>0.649711</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-05-12</td>\n      <td>No competition with ola 😅</td>\n      <td>neutral</td>\n      <td>0.514803</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-04-25</td>\n      <td>If everything goes well this will be a big hit...</td>\n      <td>positive</td>\n      <td>0.795603</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-03-31</td>\n      <td>One of the oldest auto channel in YouTube</td>\n      <td>positive</td>\n      <td>0.447022</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-02-23</td>\n      <td>1and hlf yr bck I was thinking to tk franchise...</td>\n      <td>negative</td>\n      <td>0.397321</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>628</th>\n      <td>2023-05-23</td>\n      <td>Nice🎉</td>\n      <td>positive</td>\n      <td>0.644901</td>\n    </tr>\n    <tr>\n      <th>629</th>\n      <td>2023-05-23</td>\n      <td>Cool</td>\n      <td>positive</td>\n      <td>0.491287</td>\n    </tr>\n    <tr>\n      <th>630</th>\n      <td>2023-05-23</td>\n      <td>Hum first</td>\n      <td>neutral</td>\n      <td>0.476607</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>2023-05-23</td>\n      <td>1st..😂🎉</td>\n      <td>positive</td>\n      <td>0.578617</td>\n    </tr>\n    <tr>\n      <th>632</th>\n      <td>2023-05-23</td>\n      <td>1st comment😂 1st view , 1st like 😂</td>\n      <td>neutral</td>\n      <td>0.613403</td>\n    </tr>\n  </tbody>\n</table>\n<p>633 rows × 4 columns</p>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4deca296-e2bf-440c-b4fe-f267022c991a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM Sentiment Model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
